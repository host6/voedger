/*
 * Copyright (c) 2026-present unTill Software Development Group B.V.
 */

package sqlquery

import (
	"context"
	"encoding/base64"
	"encoding/json"
	"fmt"
	"io"
	"strings"

	"github.com/blastrain/vitess-sqlparser/sqlparser"

	"github.com/voedger/voedger/pkg/appdef"
	"github.com/voedger/voedger/pkg/bus"
	"github.com/voedger/voedger/pkg/coreutils"
	"github.com/voedger/voedger/pkg/goutils/httpu"
	"github.com/voedger/voedger/pkg/goutils/strconvu"
	"github.com/voedger/voedger/pkg/goutils/timeu"
	"github.com/voedger/voedger/pkg/istructs"
	blobprocessor "github.com/voedger/voedger/pkg/processors/blobber"
)

const (
	blobFuncBlobInfo = "blobinfo"
	blobFuncBlobText = "blobtext"
	blobTextMaxBytes = 10000
)

type blobFuncDesc struct {
	funcName  string // "blobinfo" or "blobtext"
	fieldName string // blob field name
	startFrom int64  // for blobtext, optional byte offset
}

func executeBlobFunctions(
	ctx context.Context,
	blobFuncs []blobFuncDesc,
	blobHandler *blobprocessor.IRequestHandler,
	requestHandler *bus.RequestHandler,
	appQName appdef.AppQName,
	wsid istructs.WSID,
	ownerRecord appdef.QName,
	ownerID istructs.RecordID,
	token string,
	tm timeu.ITime,
) (map[string]interface{}, error) {
	localRequestSender := bus.NewIRequestSender(tm, *requestHandler)

	header := map[string]string{
		httpu.Authorization: httpu.BearerPrefix + token,
	}

	result := make(map[string]interface{})

	// Group blob functions by field name so we make one HandleRead_V2 call per field
	type fieldRequest struct {
		wantInfo  bool
		wantText  bool
		startFrom int64
	}

	var fieldOrder []string
	fieldRequests := map[string]*fieldRequest{}

	for _, bf := range blobFuncs {
		fr, exists := fieldRequests[bf.fieldName]
		if !exists {
			fr = &fieldRequest{}
			fieldRequests[bf.fieldName] = fr
			fieldOrder = append(fieldOrder, bf.fieldName)
		}
		switch bf.funcName {
		case blobFuncBlobInfo:
			fr.wantInfo = true
		case blobFuncBlobText:
			fr.wantText = true
			fr.startFrom = bf.startFrom
		}
	}

	for _, fieldName := range fieldOrder {
		fr := fieldRequests[fieldName]
		info, text, err := executeBlobRead(ctx, fieldName, fr.wantText, fr.startFrom,
			*blobHandler, localRequestSender, appQName, wsid, ownerRecord, ownerID, header)
		if err != nil {
			return nil, err
		}
		if fr.wantInfo {
			result[fmt.Sprintf("blobinfo(%s)", fieldName)] = info
		}
		if fr.wantText {
			result[fmt.Sprintf("blobtext(%s)", fieldName)] = text
		}
	}

	return result, nil
}

// executeBlobRead makes a single HandleRead_V2 call and returns both blob metadata (info)
// and blob content (text). If wantText is false, blob content is discarded.
func executeBlobRead(
	ctx context.Context,
	fieldName string,
	wantContent bool,
	startFrom int64,
	blobHandler blobprocessor.IRequestHandler,
	requestSender bus.IRequestSender,
	appQName appdef.AppQName,
	wsid istructs.WSID,
	ownerRecord appdef.QName,
	ownerID istructs.RecordID,
	header map[string]string,
) (info interface{}, text interface{}, err error) {
	capturedHeaders := make(map[string]string)
	var blobErr error

	var contentWriter io.Writer = io.Discard
	var writer *limitedBlobWriter
	if wantContent {
		writer = &limitedBlobWriter{
			skipBytes: startFrom,
			maxBytes:  blobTextMaxBytes,
		}
		contentWriter = writer
	}

	okResponseIniter := func(headersKeyValue ...string) io.Writer {
		for i := 0; i < len(headersKeyValue); i += 2 {
			capturedHeaders[headersKeyValue[i]] = headersKeyValue[i+1]
		}
		return contentWriter
	}

	errorResponder := func(statusCode int, args ...interface{}) {
		if len(args) > 0 {
			if errVal, ok := args[0].(error); ok {
				blobErr = errVal
			} else {
				blobErr = fmt.Errorf("blob read error: status %d: %v", statusCode, args[0])
			}
		} else {
			blobErr = fmt.Errorf("blob read error: status %d", statusCode)
		}
	}

	ok := blobHandler.HandleRead_V2(appQName, wsid, header, ctx,
		okResponseIniter, errorResponder,
		ownerRecord, fieldName, ownerID, requestSender)

	if !ok {
		return nil, nil, fmt.Errorf("blob processor is unavailable")
	}
	if blobErr != nil {
		return nil, nil, blobErr
	}

	// Build info metadata
	infoMap := map[string]interface{}{
		"name":     capturedHeaders[coreutils.BlobName],
		"mimetype": capturedHeaders[httpu.ContentType],
		"status":   "completed",
	}
	if sizeStr, ok := capturedHeaders[coreutils.BlobSize]; ok {
		if size, parseErr := strconvu.ParseUint64(sizeStr); parseErr == nil {
			infoMap["size"] = size
		}
	}
	info = infoMap

	// Build text content if requested
	if wantContent {
		contentType := capturedHeaders[httpu.ContentType]
		data := writer.Bytes()
		if isTextMIME(contentType) {
			text = string(data)
		} else {
			text = base64.StdEncoding.EncodeToString(data)
		}
	}

	return info, text, nil
}

func isTextMIME(contentType string) bool {
	ct := strings.ToLower(contentType)
	return strings.HasPrefix(ct, "text/") || ct == "application/json"
}

// limitedBlobWriter skips the first skipBytes bytes, then captures at most maxBytes bytes.
type limitedBlobWriter struct {
	skipBytes int64
	maxBytes  int
	skipped   int64
	buf       []byte
}

func (w *limitedBlobWriter) Write(p []byte) (int, error) {
	n := len(p)
	remaining := p

	// Skip bytes if needed
	if w.skipped < w.skipBytes {
		toSkip := w.skipBytes - w.skipped
		if int64(len(remaining)) <= toSkip {
			w.skipped += int64(len(remaining))
			return n, nil
		}
		remaining = remaining[toSkip:]
		w.skipped = w.skipBytes
	}

	// Capture up to maxBytes
	if len(w.buf) < w.maxBytes {
		space := w.maxBytes - len(w.buf)
		if len(remaining) > space {
			remaining = remaining[:space]
		}
		w.buf = append(w.buf, remaining...)
	}

	return n, nil
}

func (w *limitedBlobWriter) Bytes() []byte {
	return w.buf
}

func blobFuncResultToJSON(blobResults map[string]interface{}) (string, error) {
	bb, err := json.Marshal(blobResults)
	if err != nil {
		return "", err
	}
	return string(bb), nil
}

func parseBlobFuncExpr(funcExpr *sqlparser.FuncExpr) (blobFuncDesc, error) {
	funcName := funcExpr.Name.Lowered()
	if funcName != blobFuncBlobInfo && funcName != blobFuncBlobText {
		return blobFuncDesc{}, fmt.Errorf("unsupported function: %s", funcExpr.Name.String())
	}

	if len(funcExpr.Exprs) == 0 {
		return blobFuncDesc{}, fmt.Errorf("%s requires at least one argument (field name)", funcName)
	}

	// First argument: field name
	firstArg, ok := funcExpr.Exprs[0].(*sqlparser.AliasedExpr)
	if !ok {
		return blobFuncDesc{}, fmt.Errorf("%s: first argument must be a field name", funcName)
	}
	colName, ok := firstArg.Expr.(*sqlparser.ColName)
	if !ok {
		return blobFuncDesc{}, fmt.Errorf("%s: first argument must be a field name", funcName)
	}
	fieldName := colName.Name.String()

	bf := blobFuncDesc{
		funcName:  funcName,
		fieldName: fieldName,
	}

	// Second optional argument: startFrom (only for blobtext)
	if len(funcExpr.Exprs) > 1 {
		if funcName != blobFuncBlobText {
			return blobFuncDesc{}, fmt.Errorf("%s does not accept a second argument", funcName)
		}
		secondArg, ok := funcExpr.Exprs[1].(*sqlparser.AliasedExpr)
		if !ok {
			return blobFuncDesc{}, fmt.Errorf("%s: second argument must be a number", funcName)
		}
		sqlVal, ok := secondArg.Expr.(*sqlparser.SQLVal)
		if !ok || sqlVal.Type != sqlparser.IntVal {
			return blobFuncDesc{}, fmt.Errorf("%s: second argument (startFrom) must be an integer", funcName)
		}
		startFrom, err := strconvu.ParseInt64(string(sqlVal.Val))
		if err != nil {
			return blobFuncDesc{}, fmt.Errorf("%s: invalid startFrom value: %w", funcName, err)
		}
		if startFrom < 0 {
			return blobFuncDesc{}, fmt.Errorf("%s: startFrom must be non-negative", funcName)
		}
		bf.startFrom = startFrom
	}

	if len(funcExpr.Exprs) > 2 {
		return blobFuncDesc{}, fmt.Errorf("%s accepts at most 2 arguments", funcName)
	}

	return bf, nil
}

func mergeJSONWithBlobResults(recJSON string, blobResults map[string]interface{}) (string, error) {
	var recData map[string]interface{}
	if err := json.Unmarshal([]byte(recJSON), &recData); err != nil {
		return "", err
	}
	for k, v := range blobResults {
		recData[k] = v
	}
	bb, err := json.Marshal(recData)
	if err != nil {
		return "", err
	}
	return string(bb), nil
}
